{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08e7d83",
   "metadata": {},
   "source": [
    "# 멋진 작사가 만들기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d065e",
   "metadata": {},
   "source": [
    "먼저 사용할 라이브러리들을 호출해줍니다.\n",
    "\n",
    "데이터를 준비하고 내용을 확인해봅니다.\n",
    "\n",
    "파일을 읽기 모드로 열어준 다음 라인으로 끊어서 리스트 형태로 읽습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db07edf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\", 'It goes like this', 'The fourth, the fifth', 'The minor fall, the major lift', 'The baffled king composing Hallelujah Hallelujah', 'Hallelujah', 'Hallelujah', 'Hallelujah Your faith was strong but you needed proof']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdef80e",
   "metadata": {},
   "source": [
    "앞선 예제에서 다루어 본 것처럼 화자가 표기된 문장이나 공백인 문장이 있을 수 있습니다.\n",
    "\n",
    "그렇기에 길이가 0인 문장을 건너뛰어서 공백인 문장을 제외하고,\n",
    "\n",
    "문장의 끝부분이 :인 문장을 건너뛰어서 화자가 표기된 문장을 제외해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "810cb389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 문장 10개를 확인해 봅니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1051a1c",
   "metadata": {},
   "source": [
    "## 토큰화\n",
    "\n",
    "문장을 일정한 기준으로 나누어주는 과정을 토큰화라고 합니다.\n",
    "\n",
    "여기에서는 띄어쓰기를 기준으로 나누어 줄 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15220bee",
   "metadata": {},
   "source": [
    "토큰화하기 쉽게 전처리하는 함수를 만듭니다.\n",
    "\n",
    "함수를 살펴보면, 대소문자를 다르게 인식하는 것을 방지하기 위해 모든 문자를 소문자로 바꿉니다.\n",
    "\n",
    "문장을 정확하게 분리하기 위해서 문장 부호 양쪽에 공백을 추가합니다.\n",
    "\n",
    "공백이 여러개인 부분은 하나로 바꿔줍니다.\n",
    "\n",
    "a-zA-Z?.!,¿가 아닌 다른 문자들은 공백으로 바꿉니다.\n",
    "\n",
    "이후 다시 양쪽의 공백을 지워준 다음\n",
    "\n",
    "문장의 시작 부분에 <start>, 끝 부분에 <end>를 붙여줍니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c67be68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9ec6b",
   "metadata": {},
   "source": [
    "raw_corpus의 문장들 중 공백인 문장과 화자를 지칭하는 문장을 제외하고,\n",
    "\n",
    "문장들을 앞서 만든 전처리 함수를 통해 전처리 해준 다음, corpus 리스트에 추가해줍니다.\n",
    "\n",
    "전처리가 끝난 10개의 문장을 출력해서 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d581ea17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867caffb",
   "metadata": {},
   "source": [
    "이제 단어를 기억할 수 있는 tokenize 함수를 만들어 줍니다.\n",
    "\n",
    "12000개의 단어를 기억할 수 있게 num_words=12000으로 지정해주고,\n",
    "\n",
    "12000개의 단어에 포함되지 못한 단어는 <unk>로 바꿉니다.\n",
    "\n",
    "앞서 전처리를 했기 때문에 filter는 필요 없기에 ' '으로 합니다.\n",
    "\n",
    "전처리된 corpus를 사용해서 단어장을 만들어줍니다.\n",
    "\n",
    "tokenizer을 이용해서 corpus를 Tensor로 변환해줍니다.\n",
    "\n",
    "문장의 끝에 padding이 오게끔 padding='post'로 지정한 다음,\n",
    "\n",
    "maxlen=15로 지정합니다.(작사할 때, 긴 문장이 잘 쓰이지 않기 때문입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40867198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2639 ...    0    0    0]\n",
      " [   2   36    7 ...   43    3    0]\n",
      " ...\n",
      " [   5   22    9 ...   10 1013    3]\n",
      " [  37   15 9049 ...  877  647    3]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7efbf43973d0>\n",
      "(175749, 15)\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    print(tensor.shape)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4027aac",
   "metadata": {},
   "source": [
    "텐서 데이터의 예시를 출력해서 확인합니다.\n",
    "\n",
    "텐서 데이터는 모두 정수로 이루어짐을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c740db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5   91  297   65   57    9  969 6042]\n",
      " [   2   17 2639  873    4    8   11 6043    6  329]\n",
      " [   2   36    7   37   15  164  282   28  299    4]\n",
      " [   2   11  354   25   42    3    0    0    0    0]\n",
      " [   2    6 3604    4    6 2265    3    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:5, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07fc057",
   "metadata": {},
   "source": [
    "tokenizer에 만들어진 단어 사전을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec5f4350",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a30b53",
   "metadata": {},
   "source": [
    "앞서 확인한 텐서 데이터의 모든 행이 2로 시작했었는데 <start>가 2라서 그렇다는 것을 알 수 있겠네요.\n",
    "\n",
    "텐서를 소스와 타겟으로 분리합니다.\n",
    "\n",
    "마지막 토큰은 <pad>일 가능성이 높기에 잘라내고 소스 문장을 만들고,\n",
    "\n",
    "<start>를 잘라내고 타켓 문장을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ce8a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0]\n",
      "[  50    5   91  297   65   57    9  969 6042    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e756a0",
   "metadata": {},
   "source": [
    "데이터셋 객체를 생성합니다.\n",
    "\n",
    "텐서플로우를 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ba51c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   # + 1은 <pad>\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd97723",
   "metadata": {},
   "source": [
    "## 인공지능 학습시키기\n",
    "\n",
    "1개의 Embdding Layer, 2개의 LSTM layer, 1개의 Dense Layer로 구성된 인공지능입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88958794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256 # word 벡터의 차원 수, 단어가 추상적으로 표현되는 크기\n",
    "hidden_size = 1024\n",
    "lyricist = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc6a9a",
   "metadata": {},
   "source": [
    "한 배치를 불러와서 데이터를 모델에 넣어봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac75e821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-2.07528035e-04,  7.43569544e-05, -4.62151293e-05, ...,\n",
       "         -9.32724797e-05,  3.76116397e-04, -1.74390107e-06],\n",
       "        [-2.09052756e-04,  3.08914568e-05, -1.03920924e-04, ...,\n",
       "         -1.77570299e-04,  4.06885025e-04,  3.06748589e-05],\n",
       "        [-1.36263450e-04,  2.87887582e-04, -2.07461606e-04, ...,\n",
       "         -4.19581833e-04,  2.99590232e-04,  2.66255316e-04],\n",
       "        ...,\n",
       "        [ 6.52049319e-04, -2.42056296e-04,  7.18525378e-04, ...,\n",
       "         -1.37552066e-04,  1.30059107e-05,  7.16036826e-04],\n",
       "        [ 9.55698430e-04,  2.53730104e-05,  9.39396268e-04, ...,\n",
       "         -3.31179508e-05,  2.83836824e-04,  5.74084930e-04],\n",
       "        [ 1.02949387e-03, -3.12641896e-05,  1.29483861e-03, ...,\n",
       "          2.97021506e-05,  4.78492962e-04,  2.75904225e-04]],\n",
       "\n",
       "       [[-2.07528035e-04,  7.43569544e-05, -4.62151293e-05, ...,\n",
       "         -9.32724797e-05,  3.76116397e-04, -1.74390107e-06],\n",
       "        [-4.87710022e-05,  1.21912228e-04,  2.57920154e-04, ...,\n",
       "          1.59424017e-04,  4.64436627e-04, -1.26449850e-05],\n",
       "        [ 1.35146693e-04,  2.57798005e-04,  6.74345705e-04, ...,\n",
       "          3.48338624e-04,  7.39371171e-04, -8.33810918e-05],\n",
       "        ...,\n",
       "        [-6.19914557e-04,  6.28448557e-04,  7.90774066e-04, ...,\n",
       "         -1.41973270e-03, -1.05962215e-03,  4.41979530e-04],\n",
       "        [-7.17065181e-04,  4.44715057e-04,  1.09832373e-03, ...,\n",
       "         -1.46690872e-03, -9.98823554e-04,  4.68048209e-04],\n",
       "        [-8.71508266e-04,  4.41000419e-04,  1.37582782e-03, ...,\n",
       "         -1.56799983e-03, -9.73181101e-04,  1.97643007e-04]],\n",
       "\n",
       "       [[-2.07528035e-04,  7.43569544e-05, -4.62151293e-05, ...,\n",
       "         -9.32724797e-05,  3.76116397e-04, -1.74390107e-06],\n",
       "        [-2.63516384e-04, -6.25932007e-05, -2.56169842e-05, ...,\n",
       "         -4.55457557e-05,  5.63159934e-04, -6.45826876e-05],\n",
       "        [-2.16185494e-04,  3.22642227e-05,  2.95185309e-04, ...,\n",
       "         -2.06063487e-04,  5.35789877e-04, -1.81335286e-04],\n",
       "        ...,\n",
       "        [-1.87523162e-03,  2.80217908e-04,  1.60843052e-03, ...,\n",
       "         -1.00696215e-03,  3.35763063e-04, -1.44537748e-03],\n",
       "        [-2.55257147e-03,  6.24486536e-04,  1.48062140e-03, ...,\n",
       "         -1.13227894e-03,  3.85225576e-04, -1.43204269e-03],\n",
       "        [-3.19519220e-03,  9.83618549e-04,  1.32722489e-03, ...,\n",
       "         -1.26862375e-03,  4.35026624e-04, -1.39771204e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.07528035e-04,  7.43569544e-05, -4.62151293e-05, ...,\n",
       "         -9.32724797e-05,  3.76116397e-04, -1.74390107e-06],\n",
       "        [-6.87241554e-04, -5.27085613e-05, -1.94059510e-04, ...,\n",
       "         -1.25935898e-04,  4.96352033e-04, -1.58204875e-05],\n",
       "        [-9.01684456e-04,  2.39538008e-06, -3.22807435e-04, ...,\n",
       "         -2.24935735e-04,  4.97731613e-04,  1.23818201e-04],\n",
       "        ...,\n",
       "        [-3.47428443e-03,  1.76239875e-03,  1.24809565e-03, ...,\n",
       "         -8.99078092e-04,  9.87402396e-04, -9.95957642e-04],\n",
       "        [-4.02234169e-03,  2.02928321e-03,  1.14322000e-03, ...,\n",
       "         -1.10575918e-03,  9.56849195e-04, -1.07771507e-03],\n",
       "        [-4.50494327e-03,  2.28776014e-03,  1.02550804e-03, ...,\n",
       "         -1.29642559e-03,  9.31413029e-04, -1.12118013e-03]],\n",
       "\n",
       "       [[-2.07528035e-04,  7.43569544e-05, -4.62151293e-05, ...,\n",
       "         -9.32724797e-05,  3.76116397e-04, -1.74390107e-06],\n",
       "        [-1.03272025e-04,  3.18187580e-04, -2.03413379e-04, ...,\n",
       "         -1.78520986e-05,  7.22466910e-04, -1.71202511e-04],\n",
       "        [-2.10631028e-04,  1.30039552e-04, -4.75461304e-04, ...,\n",
       "          4.91454775e-05,  8.38550739e-04, -2.19471563e-04],\n",
       "        ...,\n",
       "        [-2.93712039e-03,  9.65351297e-04,  9.60260513e-04, ...,\n",
       "         -8.30499688e-04,  1.51618954e-03, -4.70297498e-04],\n",
       "        [-3.49829742e-03,  1.31251395e-03,  9.08840157e-04, ...,\n",
       "         -1.07527699e-03,  1.42398197e-03, -6.13411365e-04],\n",
       "        [-4.00820980e-03,  1.65289955e-03,  8.36913125e-04, ...,\n",
       "         -1.29511871e-03,  1.33838144e-03, -7.21069635e-04]],\n",
       "\n",
       "       [[-2.07528035e-04,  7.43569544e-05, -4.62151293e-05, ...,\n",
       "         -9.32724797e-05,  3.76116397e-04, -1.74390107e-06],\n",
       "        [-1.23765640e-04,  3.78310651e-04, -2.45366537e-04, ...,\n",
       "          4.23427700e-05,  6.17847487e-04,  1.09146331e-05],\n",
       "        [ 2.90190743e-04,  2.11511520e-04, -5.85021975e-04, ...,\n",
       "          3.10677162e-04,  6.47718785e-04, -6.90841480e-05],\n",
       "        ...,\n",
       "        [ 1.23763061e-03, -7.99293222e-04,  3.44548404e-04, ...,\n",
       "          9.94505128e-04,  1.20723635e-05, -7.22617202e-04],\n",
       "        [ 5.72621997e-04, -5.06575161e-04,  5.60715969e-04, ...,\n",
       "          7.39728392e-04,  3.99985402e-05, -8.23492708e-04],\n",
       "        [-2.10441052e-04, -1.14732124e-04,  6.97936106e-04, ...,\n",
       "          4.49444371e-04,  9.91172783e-05, -9.26508801e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "lyricist(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cacf228",
   "metadata": {},
   "source": [
    "모델의 최종 출력 텐서 shape를 확인할 수 있고 앞서 지정해준 값(BATCH SIZE, max_len, VOCAB_SIZE)들이 제대로 들어감을 알 수 있습니다.\n",
    "\n",
    "이후 summary를 호출해서 모델을 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0214e8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lyricist.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6026f808",
   "metadata": {},
   "source": [
    "train_test_split을 통해 train data와 test data를 분리합니다.\n",
    "\n",
    "test_size=0.2로 지정했기에 20%가 test data입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34d8aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9a5786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 14)\n",
      "Target Train: (140599, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff01a5",
   "metadata": {},
   "source": [
    "모델을 학습시킵니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe9cfef6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "686/686 [==============================] - 222s 320ms/step - loss: 3.6121\n",
      "Epoch 2/10\n",
      "686/686 [==============================] - 116s 169ms/step - loss: 3.1301\n",
      "Epoch 3/10\n",
      "686/686 [==============================] - 116s 169ms/step - loss: 2.9402\n",
      "Epoch 4/10\n",
      "686/686 [==============================] - 116s 169ms/step - loss: 2.7950\n",
      "Epoch 5/10\n",
      "686/686 [==============================] - 117s 170ms/step - loss: 2.6700\n",
      "Epoch 6/10\n",
      "686/686 [==============================] - 117s 170ms/step - loss: 2.5547\n",
      "Epoch 7/10\n",
      "686/686 [==============================] - 117s 170ms/step - loss: 2.4474\n",
      "Epoch 8/10\n",
      "686/686 [==============================] - 116s 169ms/step - loss: 2.3469\n",
      "Epoch 9/10\n",
      "686/686 [==============================] - 116s 169ms/step - loss: 2.2516\n",
      "Epoch 10/10\n",
      "686/686 [==============================] - 116s 169ms/step - loss: 2.1605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efa134485e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "lyricist.compile(loss=loss, optimizer=optimizer)\n",
    "lyricist.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27895e72",
   "metadata": {},
   "source": [
    "## 평가하기\n",
    "\n",
    "모델에 시작 문장을 입력하면 시작 문장을 바탕으로 작문을 진행하게 하는 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6172ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lyricist, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = lyricist(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ac76e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7972027f",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4520497",
   "metadata": {},
   "source": [
    "자연어처리와 관련된 프로젝트는 처음이라 신기했다.\n",
    "\n",
    "뭔가 새로운 것들을 배우고 작사 모델이 문장을 만들어 내는 것을 보니 흥미로웠다.\n",
    "\n",
    "그러나 BUT 쉽지 않았다.\n",
    "\n",
    "프로젝트를 하면 할수록 갈아 넣어야겠구나 하는 생각이 든다...\n",
    "\n",
    "계속 조금씩 어려워지는 것 같기는 하지만 힘내보겠다...!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
